{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting Housing Prices.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOO7mT0zuXSNobmkKCGCRcH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrtzcardo/Predicting_Housing_Prices/blob/main/Predicting_Housing_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XCFGVE_xYw4"
      },
      "source": [
        "Attempt to predict the median price of homes in a given Boston suburb in the\n",
        "mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on.\n",
        "Only 506 data points, split\n",
        "between 404 training samples and 102 test samples.\n",
        "Each feature in the input data\n",
        "(for example, the crime rate) has a different scale.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrRhp2YTxj0c"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "print(train_data.shape) # 404 training samples, 13 numerical features\n",
        "print(test_data.shape)  # 102 training samples, 13 numerical features\n",
        "#print(train_targets)   #  The targets are median values in thousands of dollars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em2DBHquyLaG"
      },
      "source": [
        "'''Normalizing the data'''\n",
        "'''Subtract the mean of the feature and divide by the standard deviation, \n",
        "so that the feature is centered around 0 and has a unit standard deviation'''\n",
        "\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKsBr7VH0GZy"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu', \n",
        "                         input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) #mse is common for regression probs\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SolgpWIV13mg"
      },
      "source": [
        "'''K-fold validation since little data'''\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i*num_val_samples: (i+1) * num_val_samples] #Prepares the validation data: data from partition #k\n",
        "  val_targets = train_targets[i*num_val_samples: (i+1) * num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(          #Prepares the training data: data from all other partitions\n",
        "    [train_data[:i * num_val_samples],\n",
        "    train_data[(i + 1) * num_val_samples:]],\n",
        "    axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "    [train_targets[:i * num_val_samples],\n",
        "    train_targets[(i + 1) * num_val_samples:]],\n",
        "    axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs = num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)\n",
        "\n",
        "print(all_scores)\n",
        "print(np.mean(all_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWL3Tv7L8j05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}